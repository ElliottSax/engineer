#!/usr/bin/env python3
"""
Hugging Face Space: Autocoder Analysis Worker
24/7 autonomous code analysis for the Once project

This worker continuously analyzes code, discovers issues,
and pushes findings to Hugging Face Datasets.
"""

import gradio as gr
import asyncio
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List
import os

# Hugging Face integration
from huggingface_hub import HfApi, create_repo, upload_file
from datasets import Dataset, load_dataset

# Configuration
HF_TOKEN = os.getenv("HF_TOKEN")
HF_USERNAME = os.getenv("HF_USERNAME", "autocoder")
DATASET_NAME = f"{HF_USERNAME}/autocoder-analysis-results"
SPACE_NAME = f"{HF_USERNAME}/autocoder-analysis-worker"

class AnalysisWorker:
    """24/7 Autonomous Analysis Worker"""

    def __init__(self):
        self.iteration = 0
        self.running = True
        self.status = "Initializing..."
        self.latest_results = {}
        self.hf_api = HfApi()

        # Create dataset if it doesn't exist
        try:
            self.dataset = load_dataset(DATASET_NAME, split="train")
        except (FileNotFoundError, ValueError, ConnectionError) as e:
            # Create new dataset
            self.dataset = None
            self.status = f"Creating new dataset... ({type(e).__name__})"

    async def analyze_code(self):
        """Perform code analysis iteration"""
        self.iteration += 1
        self.status = f"Running iteration {self.iteration}..."

        analysis = {
            "iteration": self.iteration,
            "timestamp": datetime.now().isoformat(),
            "analysis_type": "code_quality",
            "findings": []
        }

        # Simulate analysis (in production, this would analyze actual code)
        perspectives = [
            "Code Quality Analysis",
            "Architecture Review",
            "Performance Analysis",
            "Security Audit",
            "Dependency Analysis",
            "Test Coverage Analysis"
        ]

        current_perspective = perspectives[self.iteration % len(perspectives)]

        analysis["findings"].append({
            "perspective": current_perspective,
            "issues_found": self.iteration % 5,
            "quality_score": 0.85 + (self.iteration % 10) / 100,
            "recommendations": [
                f"Recommendation {i+1} from {current_perspective}"
                for i in range(self.iteration % 3)
            ]
        })

        self.latest_results = analysis

        # Push to Hugging Face Dataset
        await self.push_results(analysis)

        self.status = f"‚úì Iteration {self.iteration} complete"

        return analysis

    async def push_results(self, results: Dict):
        """Push results to Hugging Face Dataset"""
        if not HF_TOKEN:
            self.status += " (No HF_TOKEN - results not pushed)"
            return

        try:
            # Create dataset repo if it doesn't exist
            try:
                create_repo(
                    repo_id=DATASET_NAME,
                    token=HF_TOKEN,
                    repo_type="dataset",
                    exist_ok=True
                )
            except (ConnectionError, ValueError, PermissionError) as e:
                # Repository already exists or creation failed
                print(f"Repo creation skipped: {e}")

            # Save results locally
            results_file = Path("/tmp/analysis_results.json")
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2)

            # Upload to dataset
            upload_file(
                path_or_fileobj=str(results_file),
                path_in_repo=f"iteration_{self.iteration}.json",
                repo_id=DATASET_NAME,
                repo_type="dataset",
                token=HF_TOKEN
            )

            self.status += " (Pushed to HF Dataset)"

        except Exception as e:
            self.status += f" (Push failed: {e})"

    async def run_continuous(self):
        """Run continuous analysis loop"""
        while self.running:
            try:
                await self.analyze_code()
                await asyncio.sleep(60)  # Wait 1 minute between iterations
            except Exception as e:
                self.status = f"Error: {e}"
                await asyncio.sleep(10)

# Global worker instance
worker = AnalysisWorker()

# Gradio Interface
def get_status():
    """Get current worker status"""
    return {
        "Status": worker.status,
        "Iteration": worker.iteration,
        "Latest Results": json.dumps(worker.latest_results, indent=2) if worker.latest_results else "No results yet"
    }

def start_worker():
    """Start the worker"""
    worker.running = True
    return "Worker is already running in background!"

def stop_worker():
    """Stop the worker"""
    worker.running = False
    return "Worker stopped!"

def get_full_log():
    """Get full analysis log"""
    if not HF_TOKEN:
        return "No HF_TOKEN configured - cannot fetch logs"

    try:
        # Load all results from dataset
        dataset = load_dataset(DATASET_NAME, split="train")
        return f"Total iterations: {len(dataset)}\n\nLatest results:\n{json.dumps(worker.latest_results, indent=2)}"
    except Exception as e:
        return f"Error loading dataset: {e}"

# Build Gradio UI
with gr.Blocks(title="Autocoder Analysis Worker") as demo:
    gr.Markdown("""
    # ü§ñ Autocoder Analysis Worker

    **24/7 Autonomous Code Analysis**

    This worker continuously analyzes the Once project, discovers issues,
    and pushes findings to Hugging Face Datasets.

    ## Status
    """)

    status_display = gr.JSON(label="Current Status", value=get_status)

    with gr.Row():
        start_btn = gr.Button("‚ñ∂ Start Worker", variant="primary")
        stop_btn = gr.Button("‚è∏ Stop Worker", variant="stop")
        refresh_btn = gr.Button("üîÑ Refresh Status")

    start_output = gr.Textbox(label="Action Result")

    gr.Markdown("## Analysis Log")
    log_display = gr.Textbox(label="Full Log", lines=20)

    gr.Markdown("""
    ## Configuration

    - **Dataset**: `autocoder/autocoder-analysis-results`
    - **Update Frequency**: Every 60 seconds
    - **Analysis Perspectives**: 6 rotating perspectives

    ## Setup

    To enable dataset pushing, configure:
    1. Add `HF_TOKEN` secret in Space settings
    2. Set `HF_USERNAME` to your username (default: `autocoder`)

    ## Results

    All analysis results are pushed to the Hugging Face Dataset and can be:
    - Downloaded programmatically
    - Viewed in the Datasets UI
    - Used by other workers
    """)

    # Button actions
    start_btn.click(fn=start_worker, outputs=start_output)
    stop_btn.click(fn=stop_worker, outputs=start_output)
    refresh_btn.click(fn=get_status, outputs=status_display)

    # Auto-refresh status every 5 seconds
    demo.load(fn=get_status, outputs=status_display, every=5)

if __name__ == "__main__":
    # Auto-start worker in background
    import threading
    threading.Thread(target=lambda: asyncio.run(worker.run_continuous()), daemon=True).start()

    demo.launch(server_name="0.0.0.0", server_port=7860)
